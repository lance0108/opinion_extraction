{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '哈哈哈哈，真心不错的啊。没有异味，非常非常的柔软，吸收量很大，并且不起球，大品牌值得信赖，物流很快，辛苦啦！'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### StanfordNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.parse import CoreNLPParser, CoreNLPDependencyParser\n",
    "# parser = CoreNLPDependencyParser(url=\"http://localhost:9001\")\n",
    "parser = CoreNLPParser(url=\"http://localhost:9001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pycorenlp import StanfordCoreNLP\n",
    "# stanford = StanfordCoreNLP('http://localhost:9001')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('。', 'PU') dep ('哈哈哈哈', 'NR')\n",
      "('。', 'PU') punct ('，', 'PU')\n",
      "('。', 'PU') dep ('不错', 'VA')\n",
      "('不错', 'VA') advmod ('真心', 'AD')\n",
      "('不错', 'VA') mark ('的', 'DEC')\n",
      "('。', 'PU') discourse ('啊', 'SP')\n",
      "\n",
      "('没有', 'VE') dobj ('异味', 'NN')\n",
      "('没有', 'VE') punct ('，', 'PU')\n",
      "('没有', 'VE') conj ('大', 'VA')\n",
      "('大', 'VA') nmod:topic ('柔软', 'NN')\n",
      "('柔软', 'NN') amod ('非常', 'JJ')\n",
      "('非常', 'JJ') advmod ('非常', 'AD')\n",
      "('非常', 'JJ') mark ('的', 'DEG')\n",
      "('大', 'VA') punct ('，', 'PU')\n",
      "('大', 'VA') nsubj ('吸收量', 'NN')\n",
      "('大', 'VA') advmod ('很', 'AD')\n",
      "('大', 'VA') punct ('，', 'PU')\n",
      "('大', 'VA') conj ('不起球', 'VV')\n",
      "('不起球', 'VV') advmod ('并且', 'AD')\n",
      "('没有', 'VE') punct ('，', 'PU')\n",
      "('没有', 'VE') conj ('值得', 'VV')\n",
      "('值得', 'VV') nsubj ('品牌', 'NN')\n",
      "('品牌', 'NN') amod ('大', 'JJ')\n",
      "('值得', 'VV') dobj ('信赖', 'NN')\n",
      "('值得', 'VV') punct ('，', 'PU')\n",
      "('值得', 'VV') conj ('快', 'VA')\n",
      "('快', 'VA') nsubj ('物流', 'NN')\n",
      "('快', 'VA') advmod ('很', 'AD')\n",
      "('值得', 'VV') punct ('，', 'PU')\n",
      "('值得', 'VV') dep ('辛苦', 'VA')\n",
      "('辛苦', 'VA') discourse ('啦', 'SP')\n",
      "('没有', 'VE') punct ('！', 'PU')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parsed = parser.parse_text(text)\n",
    "for triple in parsed:\n",
    "    for (gov, rel, dep) in triple.triples():\n",
    "        print(gov, rel, dep)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in parser.parse_text(text):\n",
    "    p.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  ROOT            \n",
      "                   |               \n",
      "                   CP             \n",
      "  _________________|____________   \n",
      " |    |            NP       |   | \n",
      " |    |            |        |   |  \n",
      " |    |            CP       |   | \n",
      " |    |         ___|____    |   |  \n",
      " |    |        IP       |   |   | \n",
      " |    |        |        |   |   |  \n",
      " |    |        VP       |   |   | \n",
      " |    |    ____|___     |   |   |  \n",
      " NP   |  ADVP      VP   |  FLR  | \n",
      " |    |   |        |    |   |   |  \n",
      " NR   PU  AD       VA  DEC  SP  PU\n",
      " |    |   |        |    |   |   |  \n",
      "哈哈哈哈  ，   真心       不错   的   啊   。 \n",
      "\n",
      "                                                                             ROOT                                                                          \n",
      "                                                                              |                                                                             \n",
      "                                                                              IP                                                                           \n",
      "                  ____________________________________________________________|__________________________________________________________________________   \n",
      "                 IP                         |       |            |        |        |                |               |       |            |       |       | \n",
      "                 |                          |       |            |        |        |                |               |       |            |       |       |  \n",
      "                 VP                         |       |            |        |        |                |               |       |            |       |       | \n",
      "      ___________|______________            |       |            |        |        |                |               |       |            |       |       |  \n",
      "     |       |                  NP          |       |            |        |        |                |               |       |            |       |       | \n",
      "     |       |              ____|_______    |       |            |        |        |                |               |       |            |       |       |  \n",
      "     |       |            DNP           |   |       IP           |        |        |                IP              |       IP           |       |       | \n",
      "     |       |         ____|________    |   |    ___|____        |        |        |         _______|_______        |    ___|____        |       |       |  \n",
      "     VP      |       ADJP           |   |   |   |        VP      |        IP       |        NP              VP      |   |        VP      |      FLR      | \n",
      "  ___|___    |    ____|____         |   |   |   |    ____|___    |    ____|___     |    ____|___         ___|___    |   |    ____|___    |    ___|___    |  \n",
      " |       NP  |  ADVP      ADJP      |   NP  |   NP ADVP      VP  |  ADVP      VP   |  ADJP      NP      |       NP  |   NP ADVP      VP  |   VP      |   | \n",
      " |       |   |   |         |        |   |   |   |   |        |   |   |        |    |   |        |       |       |   |   |   |        |   |   |       |   |  \n",
      " VE      NN  PU  AD        JJ      DEG  NN  PU  NN  AD       VA  PU  AD       VV   PU  JJ       NN      VV      NN  PU  NN  AD       VA  PU  VA      SP  PU\n",
      " |       |   |   |         |        |   |   |   |   |        |   |   |        |    |   |        |       |       |   |   |   |        |   |   |       |   |  \n",
      " 没有      异味  ，   非常        非常       的   柔软  ，  吸收量  很        大   ，   并且      不起球   ，   大        品牌      值得      信赖  ，   物流  很        快   ，   辛苦      啦   ！ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Constituency tree\n",
    "for p in parser.parse_text(text):\n",
    "    p.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(text):\n",
    "    results = stanford.annotate(text, \n",
    "                                properties={'annotators': 'sentiment',\n",
    "                                            'outputFormat': 'json','timeout': '5000'})\n",
    "    for sentence in results[\"sentences\"]:\n",
    "        print(\" \".join([token[\"word\"] for token in sentence[\"tokens\"]]))\n",
    "        print(sentence[\"sentiment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "花王 的确 实 不错 ， 一直 用 这个 牌子\n",
      "Negative\n"
     ]
    }
   ],
   "source": [
    "get_sentiment(\"花王的确实不错，一直用这个牌子\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "还 不错 ， 吸水性 很 不错 ， 凑单 买 的 价格 还是 比较 实惠\n",
      "Negative\n"
     ]
    }
   ],
   "source": [
    "get_sentiment('还不错，吸水性很不错，凑单买的价格还是比较实惠')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "囤货 中 ， 东西 很 好 ！\n",
      "Negative\n",
      "先 买 了 NB 和 S 的 ， 出 月子 会 继续 买 大号 的 ！\n",
      "Negative\n",
      "包装 非常 好 ！\n",
      "Neutral\n",
      "两 包 分开 包装 ！\n",
      "Negative\n",
      "一点 都 没有 压倒 ！\n",
      "Negative\n",
      "！\n",
      "Neutral\n"
     ]
    }
   ],
   "source": [
    "get_sentiment('囤货中，东西很好！先买了NB和S的，出月子会继续买大号的！包装非常好！两包分开包装！一点都没有压倒！！')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\">The sentiment results seem to be pretty bad.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HanLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhanlp import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[哈哈哈哈/o, ，/w, 真心/d, 不错/a, 的/ude1, 啊/y, 。/w, 没有/v, 异味/n, ，/w, 非常/d, 非常/d, 的/ude1, 柔软/a, ，/w, 吸收量/n, 很大/d, ，/w, 并且/c, 不/d, 起球/n, ，/w, 大品牌/nz, 值得/v, 信赖/vn, ，/w, 物流/n, 很快/d, ，/w, 辛苦/a, 啦/y, ！/w]\n"
     ]
    }
   ],
   "source": [
    "segment = HanLP.newSegment()\n",
    "segment.enablePartOfSpeechTagging(True)\n",
    "print(segment.seg(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[非常, 值得, 大品牌, 起球, 柔软]\n"
     ]
    }
   ],
   "source": [
    "print(HanLP.extractKeyword(text, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[辛苦啦]\n"
     ]
    }
   ],
   "source": [
    "print(HanLP.extractSummary(text, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\t哈哈哈哈\t哈哈哈哈\to\to\t_\t4\t状中结构\t_\t_\n",
      "2\t，\t，\twp\tw\t_\t1\t标点符号\t_\t_\n",
      "3\t真心\t真心\td\td\t_\t4\t状中结构\t_\t_\n",
      "4\t不错\t不错\ta\ta\t_\t0\t核心关系\t_\t_\n",
      "5\t的\t的\tu\tu\t_\t4\t右附加关系\t_\t_\n",
      "6\t啊\t啊\te\ty\t_\t4\t右附加关系\t_\t_\n",
      "7\t。\t。\twp\tw\t_\t4\t标点符号\t_\t_\n",
      "8\t没有\t没有\tv\tv\t_\t4\t并列关系\t_\t_\n",
      "9\t异味\t异味\tn\tn\t_\t8\t动宾关系\t_\t_\n",
      "10\t，\t，\twp\tw\t_\t8\t标点符号\t_\t_\n",
      "11\t非常\t非常\td\td\t_\t14\t状中结构\t_\t_\n",
      "12\t非常\t非常\td\td\t_\t14\t状中结构\t_\t_\n",
      "13\t的\t的\tu\tu\t_\t12\t右附加关系\t_\t_\n",
      "14\t柔软\t柔软\ta\tan\t_\t4\t并列关系\t_\t_\n",
      "15\t，\t，\twp\tw\t_\t14\t标点符号\t_\t_\n",
      "16\t吸收量\t吸收量\tv\tv\t_\t14\t并列关系\t_\t_\n",
      "17\t很大\t很大\tj\tj\t_\t16\t动宾关系\t_\t_\n",
      "18\t，\t，\twp\tw\t_\t16\t标点符号\t_\t_\n",
      "19\t并且\t并且\tc\tc\t_\t21\t状中结构\t_\t_\n",
      "20\t不\t不\td\td\t_\t21\t状中结构\t_\t_\n",
      "21\t起球\t起球\tv\tv\t_\t14\t并列关系\t_\t_\n",
      "22\t，\t，\twp\tw\t_\t21\t标点符号\t_\t_\n",
      "23\t大品牌\t大品牌\tn\tn\t_\t24\t主谓关系\t_\t_\n",
      "24\t值得\t值得\tv\tv\t_\t21\t并列关系\t_\t_\n",
      "25\t信赖\t信赖\tv\tv\t_\t24\t动宾关系\t_\t_\n",
      "26\t，\t，\twp\tw\t_\t24\t标点符号\t_\t_\n",
      "27\t物流\t物流\tn\tn\t_\t28\t主谓关系\t_\t_\n",
      "28\t很快\t很快\td\td\t_\t30\t状中结构\t_\t_\n",
      "29\t，\t，\twp\tw\t_\t28\t标点符号\t_\t_\n",
      "30\t辛苦\t辛苦\tv\tv\t_\t24\t并列关系\t_\t_\n",
      "31\t啦\t啦\te\ty\t_\t30\t右附加关系\t_\t_\n",
      "32\t！\t！\twp\tw\t_\t4\t标点符号\t_\t_\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(HanLP.parseDependency(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m      \u001b[0mHanLP\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mType:\u001b[0m           SafeJClass\n",
       "\u001b[1;31mString form:\u001b[0m    <pyhanlp.SafeJClass object at 0x000001B3FB972BE0>\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\users\\lance\\anaconda3\\lib\\site-packages\\pyhanlp\\__init__.py\n",
       "\u001b[1;31mDocstring:\u001b[0m      <no docstring>\n",
       "\u001b[1;31mInit docstring:\u001b[0m\n",
       "JClass的线程安全版\n",
       ":param proxy: Java类的完整路径，或者一个Java对象\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyhanlp import "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SnowNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999932790580083"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from snownlp import SnowNLP\n",
    "s = SnowNLP(text)\n",
    "s.sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['哈哈哈',\n",
       " '哈',\n",
       " '，',\n",
       " '真心',\n",
       " '不错',\n",
       " '的',\n",
       " '啊',\n",
       " '。',\n",
       " '没有',\n",
       " '异味',\n",
       " '，',\n",
       " '非常',\n",
       " '非常',\n",
       " '的',\n",
       " '柔软',\n",
       " '，',\n",
       " '吸收',\n",
       " '量',\n",
       " '很',\n",
       " '大',\n",
       " '，',\n",
       " '并且',\n",
       " '不',\n",
       " '起球',\n",
       " '，',\n",
       " '大',\n",
       " '品牌',\n",
       " '值得',\n",
       " '信赖',\n",
       " '，',\n",
       " '物流',\n",
       " '很快',\n",
       " '，',\n",
       " '辛苦',\n",
       " '啦',\n",
       " '！']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('哈哈哈', 'o'),\n",
       " ('哈', 'j'),\n",
       " ('，', 'w'),\n",
       " ('真心', 'd'),\n",
       " ('不错', 'a'),\n",
       " ('的', 'u'),\n",
       " ('啊', 'y'),\n",
       " ('。', 'w'),\n",
       " ('没有', 'd'),\n",
       " ('异味', 'i'),\n",
       " ('，', 'w'),\n",
       " ('非常', 'd'),\n",
       " ('非常', 'd'),\n",
       " ('的', 'u'),\n",
       " ('柔软', 'an'),\n",
       " ('，', 'w'),\n",
       " ('吸收', 'v'),\n",
       " ('量', 'n'),\n",
       " ('很', 'd'),\n",
       " ('大', 'a'),\n",
       " ('，', 'w'),\n",
       " ('并且', 'c'),\n",
       " ('不', 'd'),\n",
       " ('起球', 'a'),\n",
       " ('，', 'w'),\n",
       " ('大', 'a'),\n",
       " ('品牌', 'n'),\n",
       " ('值得', 'v'),\n",
       " ('信赖', 'v'),\n",
       " ('，', 'w'),\n",
       " ('物流', 'n'),\n",
       " ('很快', 'd'),\n",
       " ('，', 'w'),\n",
       " ('辛苦', 'a'),\n",
       " ('啦', 'y'),\n",
       " ('！', 'w')]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(s.tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['大', '不错', '真心', '起球', '不']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.keywords(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['吸收量很大', '大品牌值得信赖', '哈哈哈哈', '真心不错的啊', '没有异味']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.summary(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9697702181250943"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = SnowNLP(\"花王的确实不错，一直用这个牌子\")\n",
    "s.sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.019393260921011346"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = SnowNLP(\"第一次在他家买的是正品，第2次买就是假货了，小兔子图案对不上了，还有里面的代码都是一个号。第一次买的小兔子能拼成一个完整的，里面的代码号都是不同的，卖家说是批次不同，只要有代码就是真的。但是我看了下质量，比前一次买的差多了，厚度也薄了很多，也没有之前吸收性好了，太失望了，再也不来了，这家货是正品和假货混合在一起 卖。\")\n",
    "s.sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0567947176648228"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = SnowNLP(\"感觉跟实体店买的不一样，打开有一股很大的味道，贴的地方做的不平有时会印到小孩，其他还好\")\n",
    "s.sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9205854232126608"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = SnowNLP('东西用着还好，但不知道真假，价格便宜的')\n",
    "s.sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
