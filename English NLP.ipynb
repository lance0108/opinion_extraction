{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.parse import CoreNLPParser, CoreNLPDependencyParser\n",
    "import nltk\n",
    "import re\n",
    "import pickle\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Hahahaha, really good. No odor, very very soft, very large absorption, and can not afford the ball, big brands are trustworthy, logistics is fast, hard work!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### StanfordNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_parser = CoreNLPDependencyParser(url=\"http://localhost:9000\")\n",
    "con_parser = CoreNLPParser(url=\"http://localhost:9000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Hahahaha', 'NNP') punct (',', ',')\n",
      "('Hahahaha', 'NNP') amod ('good', 'JJ')\n",
      "('good', 'JJ') advmod ('really', 'RB')\n",
      "('Hahahaha', 'NNP') punct ('.', '.')\n",
      "\n",
      "('work', 'NN') dep ('odor', 'NN')\n",
      "('odor', 'NN') neg ('No', 'DT')\n",
      "('odor', 'NN') punct (',', ',')\n",
      "('odor', 'NN') amod ('soft', 'JJ')\n",
      "('soft', 'JJ') advmod ('very', 'RB')\n",
      "('soft', 'JJ') advmod ('very', 'RB')\n",
      "('odor', 'NN') punct (',', ',')\n",
      "('odor', 'NN') appos ('absorption', 'NN')\n",
      "('absorption', 'NN') amod ('large', 'JJ')\n",
      "('large', 'JJ') advmod ('very', 'RB')\n",
      "('odor', 'NN') punct (',', ',')\n",
      "('odor', 'NN') cc ('and', 'CC')\n",
      "('odor', 'NN') conj ('afford', 'VB')\n",
      "('afford', 'VB') aux ('can', 'MD')\n",
      "('afford', 'VB') neg ('not', 'RB')\n",
      "('afford', 'VB') dobj ('ball', 'NN')\n",
      "('ball', 'NN') det ('the', 'DT')\n",
      "('work', 'NN') parataxis ('trustworthy', 'JJ')\n",
      "('trustworthy', 'JJ') punct (',', ',')\n",
      "('trustworthy', 'JJ') nsubj ('brands', 'NNS')\n",
      "('brands', 'NNS') amod ('big', 'JJ')\n",
      "('trustworthy', 'JJ') cop ('are', 'VBP')\n",
      "('trustworthy', 'JJ') punct (',', ',')\n",
      "('work', 'NN') nsubj ('logistics', 'NNS')\n",
      "('work', 'NN') cop ('is', 'VBZ')\n",
      "('work', 'NN') advmod ('fast', 'RB')\n",
      "('work', 'NN') punct (',', ',')\n",
      "('work', 'NN') amod ('hard', 'JJ')\n",
      "('work', 'NN') punct ('!', '.')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parsed = dep_parser.parse_text(text)\n",
    "for triple in parsed:\n",
    "    for (gov, rel, dep) in triple.triples():\n",
    "        print(gov, rel, dep)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycorenlp import StanfordCoreNLP\n",
    "stanford = StanfordCoreNLP('http://localhost:9000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(text):\n",
    "    results = stanford.annotate(text, \n",
    "                                properties={'annotators': 'sentiment',\n",
    "                                            'outputFormat': 'json','timeout': '5000'})\n",
    "    for sentence in results[\"sentences\"]:\n",
    "        print(\" \".join([token[\"word\"] for token in sentence[\"tokens\"]]))\n",
    "        print(sentence[\"sentiment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hahahaha , really good .\n",
      "Positive\n",
      "No odor , very very soft , very large absorption , and can not afford the ball , big brands are trustworthy , logistics is fast , hard work !\n",
      "Verynegative\n"
     ]
    }
   ],
   "source": [
    "get_sentiment(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good absorption\n",
      "Positive\n"
     ]
    }
   ],
   "source": [
    "get_sentiment('good absorption')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cheap price\n",
      "Negative\n"
     ]
    }
   ],
   "source": [
    "get_sentiment('cheap price')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\">The sentiment results seem to be pretty bad.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hahahaha, really good.\n",
      "No odor, very very soft, very large absorption, and can not afford the ball, big brands are trustworthy, logistics is fast, hard work!\n"
     ]
    }
   ],
   "source": [
    "for sent in doc.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hahahaha/INTJ Hahahaha/INTJ ROOT\n",
      "\n",
      " /SPACE really/ADV \n",
      "really/ADV good/ADJ advmod\n",
      "good/ADJ good/ADJ ROOT\n",
      "\n",
      " /SPACE No/DET \n",
      "No/DET odor/NOUN det\n",
      "odor/NOUN odor/NOUN ROOT\n",
      "\n",
      " /SPACE very/ADV \n",
      "very/ADV soft/ADJ advmod\n",
      "very/ADV soft/ADJ advmod\n",
      "soft/ADJ soft/ADJ ROOT\n",
      "\n",
      " /SPACE very/ADV \n",
      "very/ADV large/ADJ advmod\n",
      "large/ADJ absorption/NOUN amod\n",
      "absorption/NOUN absorption/NOUN ROOT\n",
      "\n",
      " /SPACE and/CCONJ \n",
      "and/CCONJ afford/VERB cc\n",
      "can/VERB afford/VERB aux\n",
      "not/ADV afford/VERB neg\n",
      "afford/VERB afford/VERB ROOT\n",
      "the/DET ball/NOUN det\n",
      "ball/NOUN afford/VERB dobj\n",
      "\n",
      " /SPACE big/ADJ \n",
      "big/ADJ brands/NOUN amod\n",
      "brands/NOUN are/VERB nsubj\n",
      "are/VERB are/VERB ROOT\n",
      "trustworthy/ADJ are/VERB acomp\n",
      "\n",
      " /SPACE logistics/NOUN \n",
      "logistics/NOUN is/VERB nsubj\n",
      "is/VERB is/VERB ROOT\n",
      "fast/ADJ is/VERB acomp\n",
      "\n",
      " /SPACE hard/ADJ \n",
      "hard/ADJ work/NOUN amod\n",
      "work/NOUN work/NOUN ROOT\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "    annotation = nlp(sentence)\n",
    "    for word in annotation:\n",
    "        print(str(word)+\"/\"+word.pos_, str(word.head)+\"/\"+word.head.pos_, word.dep_)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse all English texts using Stanford Parser. The parser does a better job in collapsing indirect dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>REVIEW_TEXT_CN</th>\n",
       "      <th>REVIEW_TEXT_EN</th>\n",
       "      <th>ONLINE_STORE</th>\n",
       "      <th>BRAND</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>MONTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>花王的确实不错，一直用这个牌子</td>\n",
       "      <td>Kao is really good, always use this brand</td>\n",
       "      <td>tmall</td>\n",
       "      <td>Merries</td>\n",
       "      <td>2016</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>哈哈哈哈，真心不错的啊。没有异味，非常非常的柔软，吸收量很大，并且不起球，大品牌值得信赖，物...</td>\n",
       "      <td>Hahahaha, really good. No odor, very very soft...</td>\n",
       "      <td>suning</td>\n",
       "      <td>Huggies</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>还不错，吸水性很不错，凑单买的价格还是比较实惠</td>\n",
       "      <td>Not bad, water absorption is very good, the pr...</td>\n",
       "      <td>JINGDONG</td>\n",
       "      <td>Merries</td>\n",
       "      <td>2018</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>囤货中，东西很好！先买了NB和S的，出月子会继续买大号的！包装非常好！两包分开包装！一点都没...</td>\n",
       "      <td>In the goods, things are very good! First boug...</td>\n",
       "      <td>tmall</td>\n",
       "      <td>Merries</td>\n",
       "      <td>2014</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>感觉跟实体店买的不一样，打开有一股很大的味道，贴的地方做的不平有时会印到小孩，其他还好</td>\n",
       "      <td>It feels different from the one bought in t...</td>\n",
       "      <td>jingdong</td>\n",
       "      <td>Pampers</td>\n",
       "      <td>2016</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      REVIEW_TEXT_CN  \\\n",
       "0                                    花王的确实不错，一直用这个牌子   \n",
       "1  哈哈哈哈，真心不错的啊。没有异味，非常非常的柔软，吸收量很大，并且不起球，大品牌值得信赖，物...   \n",
       "2                            还不错，吸水性很不错，凑单买的价格还是比较实惠   \n",
       "3  囤货中，东西很好！先买了NB和S的，出月子会继续买大号的！包装非常好！两包分开包装！一点都没...   \n",
       "4        感觉跟实体店买的不一样，打开有一股很大的味道，贴的地方做的不平有时会印到小孩，其他还好   \n",
       "\n",
       "                                      REVIEW_TEXT_EN ONLINE_STORE    BRAND  \\\n",
       "0          Kao is really good, always use this brand        tmall  Merries   \n",
       "1  Hahahaha, really good. No odor, very very soft...       suning  Huggies   \n",
       "2  Not bad, water absorption is very good, the pr...     JINGDONG  Merries   \n",
       "3  In the goods, things are very good! First boug...        tmall  Merries   \n",
       "4     It feels different from the one bought in t...     jingdong  Pampers   \n",
       "\n",
       "   YEAR  MONTH  \n",
       "0  2016     11  \n",
       "1  2017      1  \n",
       "2  2018      2  \n",
       "3  2014      8  \n",
       "4  2016      3  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data = pd.read_csv(r\"C:\\Dropbox\\_projects\\PG\\ds-nlp-interview-question_v2.csv\")\n",
    "print(len(df_data))\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data[\"id\"] = list(range(len(df_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 15min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "all_parsed = {}\n",
    "dep_parser = CoreNLPDependencyParser(url=\"http://localhost:9000\")\n",
    "for i, row in df_data.iterrows():\n",
    "    parsed = dep_parser.parse_text(row[\"REVIEW_TEXT_EN\"])\n",
    "    record = []\n",
    "    for triple in parsed:\n",
    "        record.append(list(triple.triples()))\n",
    "    all_parsed[row[\"id\"]] = record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36000"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(all_parsed, open(\"deps_en.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consolidate tags\n",
    "A = set([\"JJ\", \"JJR\", \"JJS\"])\n",
    "R = set([\"RB\", \"RBR\", \"RBS\"])\n",
    "V = set([\"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"])\n",
    "N = set([\"NN\", \"NNS\"])\n",
    "def replace_tag(tag):\n",
    "    if tag in A:\n",
    "        return \"A\"\n",
    "    if tag in R:\n",
    "        return \"R\"\n",
    "    if tag in V:\n",
    "        return \"V\"\n",
    "    if tag in N:\n",
    "        return \"N\"\n",
    "    return tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_parsed2 = {}\n",
    "\n",
    "for doc_id, sentences in all_parsed.items():\n",
    "    new_sentences = []\n",
    "    for sentence in sentences:\n",
    "        new_sentence = []\n",
    "        for gov, rel, dep in sentence:\n",
    "            new_sentence.append(((gov[0].lower(), replace_tag(gov[1])),\n",
    "                                rel,\n",
    "                                (dep[0].lower(), replace_tag(dep[1])))) \n",
    "        new_sentences.append(new_sentence)\n",
    "    all_parsed2[doc_id] = new_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(('good', 'A'), 'nsubj', ('kao', 'NNP')),\n",
       "  (('good', 'A'), 'cop', ('is', 'V')),\n",
       "  (('good', 'A'), 'advmod', ('really', 'R')),\n",
       "  (('good', 'A'), 'punct', (',', ',')),\n",
       "  (('good', 'A'), 'parataxis', ('use', 'V')),\n",
       "  (('use', 'V'), 'advmod', ('always', 'R')),\n",
       "  (('use', 'V'), 'dobj', ('brand', 'N')),\n",
       "  (('brand', 'N'), 'det', ('this', 'DT'))]]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_parsed2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(all_parsed2, open(\"deps_en2.pkl\", \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
